```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE
    )
```


```{r}
# LIME FEATURE EXPLANATION ----

# 1. Setup ----

# Load Libraries 
library(tidymodels)
library(magrittr)
library(dplyr)
library(sjmisc)
library(magrittr)
library(haven)
library(sjlabelled)
library(rsample)
library(recipes)
library(rstanarm)
library(broom.mixed)
library(h2o)
library(readxl)
library(tidyverse)
library(tidyquant)
library(lime)

```

```{r}

# Load Data
employee_attrition_tbl <- read_csv("datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv")
definitions_raw_tbl    <- read_excel("data_definitions.xlsx", sheet = 1, col_names = FALSE)

# Processing Pipeline
source("00_Scripts/data_processing_pipeline.R")

employee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)

# Split into test and train
set.seed(seed = 1113)
split_obj <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)

# Assign training and test data
train_readable_tbl <- training(split_obj)
test_readable_tbl  <- testing(split_obj)

# ML Preprocessing Recipe 
recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_mutate_at(c("JobLevel", "StockOptionLevel"), fn = as.factor) %>% 
  prep()

recipe_obj

train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
test_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)

```

```{r}


# 2. Models ----

h2o.init()

# Splitting the data for validation df
split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.75), seed = 1234)
train_h2o <- split_h2o[[1]]
valid_h2o <- split_h2o[[2]]
test_h2o  <- as.h2o(test_tbl)
# Predictors
y <- "EducationField"
x <- setdiff(names(train_h2o), y)

# AutoML Model
h2o_models_automl <- h2o.automl(
  x = x,
  y = y,
  training_frame    = train_h2o,
  validation_frame  = valid_h2o,
  leaderboard_frame = test_h2o,
  max_runtime_secs  = 120,
  nfolds            = 5 
)
typeof(h2o_models_automl)
slotNames(h2o_models_automl)
h2o_models_automl@leaderboard 
h2o_models_automl@leader
```

```{r}
#h2o.getModel("XGBoost_grid_1_AutoML_8_20240613_54046_model_2") %>% 
#  h2o.saveModel(path = "h20_models/")
automl_leader <- h2o.loadModel("h20_models/XGBoost_grid_1_AutoML_8_20240613_54046_model_2")
automl_leader
```

```{r}

# 3. LIME ----

# 3.1 Making Predictions ----
#
predictions_tbl <- automl_leader %>% 
     h2o.predict(newdata = as.h2o(test_tbl)) %>%
     as.tibble() %>%
     bind_cols(
         test_tbl %>%
             select(EducationField)
     )
 
 predictions_tbl
 test_tbl %>%
   slice(1) %>%
   glimpse()
```

```{r} 
# 3.2 Single Explanation ----

 explainer <- train_tbl %>%
   select(-Attrition) %>%
   lime(
     model           = automl_leader,
     bin_continuous  = TRUE,
     n_bins          = 4,
     quantile_bins   = TRUE
   )
 
 explainer
```

```{r}
?lime::explain
 
 explanation <- test_tbl %>%
   slice(1) %>%
   select(-Attrition) %>%
   lime::explain(
     
     # Pass our explainer object
     explainer = explainer,
     # Because it is a binary classification model: 1
     n_labels   = 1,
     # number of features to be returned
     n_features = 8,
     # number of localized linear models
     n_permutations = 5000,
     # Let's start with 1
     kernel_width   = 1
   )
 
 explanation
 
 explanation %>%
   as.tibble() %>%
   select(feature:prediction) 
 
 g <- plot_features(explanation = explanation, ncol = 1)
 
 plot_features(explanation = explanation, ncol = 1)
```
 
```{r} 
# 3.3 Multiple Explanations ----
 
 explanation <- test_tbl %>%
   slice(1:20) %>%
   select(-Attrition) %>%
   lime::explain(
     explainer = explainer,
     n_labels   = 1,
     n_features = 8,
     n_permutations = 5000,
     kernel_width   = 0.5
   )
 
 explanation %>%
   as.tibble()
 
 plot_features(explanation, ncol = 4)
 
 plot_explanations(explanation)
```


# Challenge part 1 ----
```{r}
 explanation %>% 
   as.tibble()
 
 case_1 <- explanation %>%
   filter(case == 1)
 
 case_1 %>%
   plot_features()
 
 case_1 %>%
   ggplot(aes(feature_weight, feature)) +
   geom_col(fill = "#1a2c50") +
   geom_smooth(method = "lm", se = FALSE) +
   scale_fill_manual(values = c("steelblue", "firebrick"), drop = FALSE) +
   labs(
     title = ("Model explanation"),
     x = "Weight",
     y = "Feature"
   ) +
   theme_tq_dark()
```

# Challenge part 2 ----
```{r}
 explanation %>% ggplot(aes_(~case, ~feature_desc)) +
   geom_tile(aes_(fill = ~feature_weight)) + 
   scale_x_discrete("Case", expand = c(0, 0)) +
   scale_y_discrete("Feature", expand = c(0, 0)) +
   scale_fill_gradient2("Feature\nweight", low = "firebrick", mid = "#f7f7f7", high = "steelblue") +
   theme(panel.border = element_rect(fill = NA,
                                     colour = "grey60",
                                     size = 1),
         panel.grid = element_blank(),
         legend.position = "right",
         axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
   facet_wrap(~label)
```